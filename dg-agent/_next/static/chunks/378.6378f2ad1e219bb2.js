"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[378],{1020:function(e,n,t){t.a(e,async function(e,o){try{t.d(n,{n:function(){return useAgentChat}});var a,s,r=t(7193),i=t(1798),l=t(2809),c=e([r,l]);function useAgentChat(e){let{apiKey:n,modelName:t,toolsJson:o,toolsContext:a,extraTools:s}=e,[c,d]=(0,i.useState)([{role:"system",content:e.sysPrompt}]),[u,m]=(0,i.useState)(0),[h,p]=(0,i.useState)(null),[g,f]=(0,i.useState)(null);async function addMessages(e){if(!h||2!==u)throw console.error("Agent not ready"),Error("Agent not ready");d([...c,...e])}return(0,i.useEffect)(()=>{(async function(){try{let e=(0,l.I)(o,a);if(s)for(let n of s)e.add(new r.UA(new r.mU(n.name,n.description,n.parameters),n.action));let i=new r.GT(n,t,e);p(i),m(2)}catch(e){console.error(e)}})()},[n,t,o,s]),(0,i.useEffect)(()=>{(async function(){if(h&&2===u&&0!==c.length&&"user"===c[c.length-1].role){m(3),console.info("Running agent");let e=h.run_react(c.map(e=>new r.v0(e)));f(e)}})()},[h,c,u]),(0,i.useEffect)(()=>{(async function(){if(g)try{let e=g.getReader();console.info("Reading next message");let{done:n,value:t}=await e.read();if(e.releaseLock(),console.info("Finished reading next message"),n){f(null),m(2);return}d([...c,t])}catch(e){console.error(e)}})()},[g,c]),{messages:c,isReady:2===u,addMessages}}[r,l]=c.then?(await c)():c,(a=s||(s={}))[a.Initializing=0]="Initializing",a[a.Loading=1]="Loading",a[a.Ready=2]="Ready",a[a.Running=3]="Running",o()}catch(e){o(e)}})},6157:function(e,n,t){t.a(e,async function(e,o){try{t.r(n),t.d(n,{default:function(){return ChatPage}});var a=t(8896),s=t(3183),r=t(1791),i=t(2105),l=t(1798),c=t(95),d=e([s]);function ChatPage(){let[e,n]=(0,l.useState)(null),t=(0,a.jsx)("span",{className:"",children:"Loading tools..."});e&&(t=(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)("span",{className:"",children:"Loaded Tools:"}),(0,a.jsx)("ul",{className:"ml-8",children:e.map(e=>(0,a.jsxs)("li",{children:[e.name,(0,a.jsx)("span",{className:"ml-2",children:e.description})]},e.uuid))})]}));let o=(0,a.jsxs)("div",{className:"p-4 md:p-8 rounded bg-[#25252d] w-full max-h-[85%] overflow-hidden",children:[(0,a.jsx)("h1",{className:"text-3xl md:text-4xl mb-4",children:"Agri chat - Agent test bot"}),(0,a.jsxs)("ul",{children:[(0,a.jsxs)("li",{className:"hidden text-l md:block",children:["\uD83D\uDCBB",(0,a.jsx)("span",{className:"ml-2",children:"Adjust the system prompt in the System Prompts page."})]}),(0,a.jsxs)("li",{children:["\uD83D\uDEE0️ Adjust tools in the Toolkits page. ",(0,a.jsx)("br",{}),t]})]})]}),[d,u]=(0,i._)(r.Ye,null),m=null;d||(m="No OpenAI key set");let[h,p]=(0,i._)(r.nR,null);h||(m="No model name set");let[g,f]=(0,i._)(r.dc,null);if(g||(m="No system prompt set"),(0,l.useEffect)(()=>{(0,c.V)().then(e=>{n(e)})},[]),m)return(0,a.jsxs)("div",{className:"p-4 md:p-8 rounded bg-[#35252d] w-full",children:[(0,a.jsx)("h1",{className:"text-3xl md:text-4xl mb-4",children:"Config error"}),(0,a.jsx)("p",{children:m})]});if(!e)return o;let w=JSON.stringify(e);return(0,a.jsx)(s.z,{emptyStateComponent:o,placeholder:"Test the agri agent...",titleText:"Agri Chat",emoji:"\uD83E\uDD9C",toolsJson:w,showIntermediateStepsToggle:!0,openAIApiKey:d,modelName:h,sysPrompt:g})}s=(d.then?(await d)():d)[0],o()}catch(e){o(e)}})},2961:function(e,n,t){t.d(n,{x:function(){return ChatMessageBubble}});var o=t(8896);function ChatMessageBubble(e){let n="user"===e.role?"bg-sky-600":"bg-slate-50 text-black",t="user"===e.role?"ml-auto":"mr-auto",a="user"===e.role?"\uD83E\uDDD1":e.aiEmoji;return(0,o.jsxs)("div",{className:"".concat(t," ").concat(n," rounded px-4 py-1 max-w-[80%] mb-2 flex"),children:[(0,o.jsx)("div",{className:"mr-2",children:a}),(0,o.jsx)("div",{className:"whitespace-pre-wrap flex flex-col",children:(0,o.jsx)("span",{children:e.content})})]})}},3183:function(e,n,t){t.a(e,async function(e,o){try{t.d(n,{z:function(){return ChatWindow}});var a=t(8896),s=t(2322);t(7157);var r=t(1798),i=t(2961),l=t(9273),c=t(8323),d=t(2105),u=t(1791),m=t(7644),h=t(1648),p=t(1020),g=t(7932),f=t(1430).Buffer,w=e([p]);function ChatWindow(e){let n=(0,r.useRef)(null),t=(0,r.useRef)(null),{emptyStateComponent:o,placeholder:w,titleText:y="An LLM",showIngestForm:v,showIntermediateStepsToggle:x,emoji:b}=e,[C,k]=(0,r.useState)(!1),[j,A]=(0,r.useState)(!1),S=v&&(0,a.jsx)(l.q,{}),_=x&&(0,a.jsxs)("div",{children:[(0,a.jsx)("input",{type:"checkbox",id:"show_intermediate_steps",name:"show_intermediate_steps",checked:C,onChange:e=>k(e.target.checked)}),(0,a.jsx)("label",{htmlFor:"show_intermediate_steps",children:" Show intermediate steps"})]}),[N,T]=(0,r.useState)(""),[D,M]=(0,d._)(u.RM,""),{username:I,token:R}=(0,h.yH)(),[L,O]=(0,r.useState)(null),E=(0,r.useMemo)(()=>({ensureUpload:async function(){let e=new g.W;return O(e),(0,s.Am)("Please upload a file to answer the question.",{theme:"dark",autoClose:!1}),await e.promise()}}),[]),B={apiKey:e.openAIApiKey,modelName:e.modelName,toolsJson:e.toolsJson,toolsContext:E,sysPrompt:e.sysPrompt},{isReady:q,messages:Q,addMessages:F}=(0,p.n)(B);async function sendMessage(e){var t;e.preventDefault(),n.current&&n.current.classList.add("grow"),Q.length||await new Promise(e=>setTimeout(e,300)),((t=!q,void 0!==t)?t:j)||(F([{content:N,role:"user"}]),T(""))}function reindent(e,n){let t=" ".repeat(n);return e.split("\n").map(e=>t+e).join("\n")}async function createMarkdown(){var n,o;let a=new Date,s=a.toISOString().replace(/:/g,"-").split(".")[0],r=null!==(o=null===(n=t.current)||void 0===n?void 0:n.value)&&void 0!==o?o:"chat-".concat(s),i=Q.map(e=>{var n;let t=e.role,o=null!==(n=e.content)&&void 0!==n?n:"";if("assistant"==e.role)e.content?(t="output",o=e.content):e.tool_calls&&(t="assistant/tool-call",o=JSON.stringify(e.tool_calls,null,2),o=reindent(o,1),o="``` json\n"+o+"\n```\n",o="\n"+reindent(o,2));else if("system"==e.role)t="system",o=e.content;else if("tool"==e.role)t="tool",o="Id: ".concat(e.tool_call_id,"\n  Observation: ").concat(reindent(e.content,2));else if("user"==e.role)t="user",o=e.content;else throw Error("Unknown message role: ".concat(t));return"- **".concat(t,"**: ").concat(o)}).join("\n"),l=JSON.parse(e.toolsJson),c=l.map(e=>"\n### Tool: ".concat(e.name,"\n\n").concat(e.description,"\n\nParameters schema: \n``` json\n").concat(JSON.stringify(e.parameters,null,2),"\n```\n\nAction:\n``` js\n").concat(e.action,"\n```\n    ")).join("\n"),d="\n\n# ".concat(r,"\n\n- Model: ").concat(e.modelName,"\n- Date: ").concat(s,"\n\n# Chat messages\n").concat(i,"\n\n# System prompt\n").concat(e.sysPrompt,"\n\n# Tools\n").concat(c,"\n"),u="".concat(r,"-").concat(s,".md");return{outputText:d,title:r,dateString:s,filename:u}}async function saveMessages(){let{outputText:e,filename:n}=await createMarkdown(),t=new Blob([e],{type:"text/markdown"}),o=URL.createObjectURL(t),a=document.createElement("a");a.href=o,a.download=n,a.click()}async function exportMessages(){if(!D)return null;try{let{repo:e,branch:n,filename:t}=await exportMessagesToGithub();(0,s.Am)("Pushed ".concat(t," to ").concat(e,"/").concat(n))}catch(e){(0,s.Am)("Error: GH Export: ".concat(e),{theme:"dark"})}}async function exportMessagesToGithub(){let{outputText:e,filename:n}=await createMarkdown(),t=new m.v({auth:R}),{owner:o,repo:a,branch:s}=(0,h.v7)(D),r="runs/".concat(s,"/").concat(n);return await t.repos.createOrUpdateFileContents({owner:o,repo:a,path:r,message:"at-ui: ".concat(n),content:f.from(e).toString("base64"),branch:s}),{owner:o,repo:a,branch:s,path:r,filename:n}}let P=null;if(Q.length>0){let e=null;P=Q.map((n,t)=>{var o;if("tool"===n.role){let o=null;return e&&(o=e[n.tool_call_id]),(0,a.jsx)(c.t,{tool:n.tool_call_id,observation:n.content,toolInput:o},t)}if("system"===n.role)return(0,a.jsx)(c.t,{tool:"SYSTEM",toolInput:null,observation:n.content},t);if("assistant"==n.role){if(n.content)return e=null,(0,a.jsx)(i.x,{role:"assistant",content:n.content,aiEmoji:b,sources:[]},t);if(n.tool_calls)return e=n.tool_calls.reduce((e,n)=>(e[n.tool_call_id]={name:n.name,arguments:n.arguments},e),{}),(0,a.jsx)(c.t,{tool:"ASSISTANT",toolInput:n.tool_calls,observation:n.content},t);throw Error("Unreadable assistant message: ".concat(n))}return(0,a.jsx)(i.x,{role:n.role,content:null!==(o=n.content)&&void 0!==o?o:null,aiEmoji:b,sources:[]},t)}).reverse()}function handleFileSelect(e){var n;let t=null===(n=e.target.files)||void 0===n?void 0:n[0];t&&L&&L.put(t)}let W=e.toolsJson?JSON.parse(e.toolsJson).length:0;return(0,a.jsxs)("div",{className:"flex flex-col items-center p-4 md:p-8 rounded grow overflow-hidden ".concat(Q.length>0?"border":""),children:[(0,a.jsxs)("h2",{className:"".concat(Q.length>0?"":"hidden"," text-2xl"),children:[b," ",y," ","+ ".concat(W," tools")]}),0===Q.length?o:"",(0,a.jsxs)("div",{className:"flex flex-col-reverse w-full mb-4 overflow-auto transition-[flex-grow] ease-in-out",ref:n,children:[" ",P]}),0===Q.length&&S,(0,a.jsxs)("form",{onSubmit:sendMessage,className:"flex w-full flex-col",children:[(0,a.jsx)("div",{className:"flex",children:_}),(0,a.jsxs)("div",{className:"flex w-full mt-4",children:[(0,a.jsx)("input",{className:"grow mr-8 p-4 rounded",value:N,onChange:e=>T(e.target.value),placeholder:null!=w?w:"What's it like to be a pirate?"}),(0,a.jsxs)("label",{htmlFor:"file-input",className:"mx-2 px-8 py-4 bg-sky-600 rounded cursor-pointer",children:["\uD83D\uDDBC",(0,a.jsx)("input",{type:"file",className:"hidden",id:"file-input",onChange:handleFileSelect})]}),(0,a.jsxs)("button",{type:"submit",className:"shrink-0 px-8 py-4 bg-sky-600 rounded w-28",children:[(0,a.jsxs)("div",{role:"status",className:"".concat(q?"hidden":""," flex justify-center"),children:[(0,a.jsxs)("svg",{"aria-hidden":"true",className:"w-6 h-6 text-white animate-spin dark:text-white fill-sky-800",viewBox:"0 0 100 101",fill:"none",xmlns:"http://www.w3.org/2000/svg",children:[(0,a.jsx)("path",{d:"M100 50.5908C100 78.2051 77.6142 100.591 50 100.591C22.3858 100.591 0 78.2051 0 50.5908C0 22.9766 22.3858 0.59082 50 0.59082C77.6142 0.59082 100 22.9766 100 50.5908ZM9.08144 50.5908C9.08144 73.1895 27.4013 91.5094 50 91.5094C72.5987 91.5094 90.9186 73.1895 90.9186 50.5908C90.9186 27.9921 72.5987 9.67226 50 9.67226C27.4013 9.67226 9.08144 27.9921 9.08144 50.5908Z",fill:"currentColor"}),(0,a.jsx)("path",{d:"M93.9676 39.0409C96.393 38.4038 97.8624 35.9116 97.0079 33.5539C95.2932 28.8227 92.871 24.3692 89.8167 20.348C85.8452 15.1192 80.8826 10.7238 75.2124 7.41289C69.5422 4.10194 63.2754 1.94025 56.7698 1.05124C51.7666 0.367541 46.6976 0.446843 41.7345 1.27873C39.2613 1.69328 37.813 4.19778 38.4501 6.62326C39.0873 9.04874 41.5694 10.4717 44.0505 10.1071C47.8511 9.54855 51.7191 9.52689 55.5402 10.0491C60.8642 10.7766 65.9928 12.5457 70.6331 15.2552C75.2735 17.9648 79.3347 21.5619 82.5849 25.841C84.9175 28.9121 86.7997 32.2913 88.1811 35.8758C89.083 38.2158 91.5421 39.6781 93.9676 39.0409Z",fill:"currentFill"})]}),(0,a.jsx)("span",{className:"sr-only",children:"Loading..."})]}),(0,a.jsx)("span",{className:!q||j?"hidden":"",children:"Send"})]})]}),(0,a.jsxs)("div",{className:"flex w-full mt-4",children:[(0,a.jsx)("input",{className:"grow mr-8 p-4 rounded",placeholder:"Chat Title...",ref:t}),(0,a.jsx)("button",{type:"button",className:"shrink-0 px-8 py-4 bg-sky-600 rounded w-28",onClick:saveMessages,children:"Save"}),I&&(0,a.jsx)("button",{type:"button",className:"shrink-0 ml-2 px-8 py-4 bg-sky-600 rounded w-28",onClick:exportMessages,children:"Export"})]})]}),(0,a.jsx)(s.Ix,{})]})}p=(w.then?(await w)():w)[0],o()}catch(e){o(e)}})},8323:function(e,n,t){t.d(n,{t:function(){return IntermediateStep}});var o=t(8896),a=t(1798);function IntermediateStep(e){let[n,t]=(0,a.useState)(!1),{tool:s,toolInput:r,observation:i}=e;return(0,o.jsxs)("div",{className:"ml-auto bg-green-600 rounded px-2 py-1 max-w-[80%] mb-2 whitespace-pre-wrap flex flex-col cursor-pointer",children:[(0,o.jsxs)("div",{className:"text-right ".concat(n?"w-full":""),onClick:e=>t(!n),children:[(0,o.jsxs)("code",{className:"mr-2 bg-slate-600 px-2 py-1 rounded hover:text-blue-600",children:["\uD83D\uDEE0️ ",(0,o.jsx)("b",{children:s})]}),(0,o.jsx)("span",{className:n?"hidden":"",children:"\uD83D\uDD3D"}),(0,o.jsx)("span",{className:n?"":"hidden",children:"\uD83D\uDD3C"})]}),(0,o.jsxs)("div",{className:"overflow-scroll max-h-[0px] transition-[max-height] ease-in-out ".concat(n?"max-h-[360px]":""),children:[r&&(0,o.jsx)("div",{className:"bg-slate-600 rounded p-4 mt-1 max-w-0 ".concat(n?"max-w-full":"transition-[max-width] delay-100"),children:(0,o.jsxs)("code",{className:"opacity-0 max-h-[100px] overflow-auto transition ease-in-out delay-150 ".concat(n?"opacity-100":""),children:["Tool Input:",(0,o.jsx)("br",{}),(0,o.jsx)("br",{}),JSON.stringify(r,null,2)]})}),i&&(0,o.jsx)("div",{className:"bg-slate-600 rounded p-4 mt-1 max-w-0 ".concat(n?"max-w-full":"transition-[max-width] delay-100"),children:(0,o.jsx)("code",{className:"opacity-0 max-h-[260px] overflow-auto transition ease-in-out delay-150 ".concat(n?"opacity-100":""),children:i})})]})]})}},9273:function(e,n,t){t.d(n,{q:function(){return UploadDocumentsForm}});var o=t(8896),a=t(1798);function UploadDocumentsForm(){let[e,n]=(0,a.useState)(!1),[t,s]=(0,a.useState)('# QA and Chat over Documents\n\nChat and Question-Answering (QA) over `data` are popular LLM use-cases.\n\n`data` can include many things, including:\n\n* `Unstructured data` (e.g., PDFs)\n* `Structured data` (e.g., SQL)\n* `Code` (e.g., Python)\n\nBelow we will review Chat and QA on `Unstructured data`.\n\n![intro.png](/img/qa_intro.png)\n\n`Unstructured data` can be loaded from many sources.\n\nCheck out the [document loader integrations here](/docs/modules/data_connection/document_loaders/) to browse the set of supported loaders.\n\nEach loader returns data as a LangChain `Document`.\n\n`Documents` are turned into a Chat or QA app following the general steps below:\n\n* `Splitting`: [Text splitters](/docs/modules/data_connection/document_transformers/) break `Documents` into splits of specified size\n* `Storage`: Storage (e.g., often a [vectorstore](/docs/modules/data_connection/vectorstores/)) will house [and often embed](https://www.pinecone.io/learn/vector-embeddings/) the splits\n* `Retrieval`: The app retrieves splits from storage (e.g., often [with similar embeddings](https://www.pinecone.io/learn/k-nearest-neighbor/) to the input question)\n* `Output`: An [LLM](/docs/modules/model_io/models/llms/) produces an answer using a prompt that includes the question and the retrieved splits\n\n![flow.jpeg](/img/qa_flow.jpeg)\n\n## Quickstart\n\nLet\'s load this [blog post](https://lilianweng.github.io/posts/2023-06-23-agent/) on agents as an example `Document`.\n\nWe\'ll have a QA app in a few lines of code.\n\nFirst, set environment variables and install packages required for the guide:\n\n```shell\n> yarn add cheerio\n# Or load env vars in your preferred way:\n> export OPENAI_API_KEY="..."\n```\n\n## 1. Loading, Splitting, Storage\n\n### 1.1 Getting started\n\nSpecify a `Document` loader.\n\n```typescript\n// Document loader\nimport { CheerioWebBaseLoader } from "langchain/document_loaders/web/cheerio";\n\nconst loader = new CheerioWebBaseLoader(\n  "https://lilianweng.github.io/posts/2023-06-23-agent/"\n);\nconst data = await loader.load();\n```\n\nSplit the `Document` into chunks for embedding and vector storage.\n\n\n```typescript\nimport { RecursiveCharacterTextSplitter } from "langchain/text_splitter";\n\nconst textSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 500,\n  chunkOverlap: 0,\n});\n\nconst splitDocs = await textSplitter.splitDocuments(data);\n```\n\nEmbed and store the splits in a vector database (for demo purposes we use an unoptimized, in-memory example but you can [browse integrations here](/docs/modules/data_connection/vectorstores/integrations/)):\n\n\n```typescript\nimport { OpenAIEmbeddings } from "langchain/embeddings/openai";\nimport { MemoryVectorStore } from "langchain/vectorstores/memory";\n\nconst embeddings = new OpenAIEmbeddings();\n\nconst vectorStore = await MemoryVectorStore.fromDocuments(splitDocs, embeddings);\n```\n\nHere are the three pieces together:\n\n![lc.png](/img/qa_data_load.png)\n\n### 1.2 Going Deeper\n\n#### 1.2.1 Integrations\n\n`Document Loaders`\n\n* Browse document loader integrations [here](/docs/modules/data_connection/document_loaders/).\n\n* See further documentation on loaders [here](/docs/modules/data_connection/document_loaders/).\n\n`Document Transformers`\n\n* All can ingest loaded `Documents` and process them (e.g., split).\n\n* See further documentation on transformers [here](/docs/modules/data_connection/document_transformers/).\n\n`Vectorstores`\n\n* Browse vectorstore integrations [here](/docs/modules/data_connection/vectorstores/integrations/).\n\n* See further documentation on vectorstores [here](/docs/modules/data_connection/vectorstores/).\n\n## 2. Retrieval\n\n### 2.1 Getting started\n\nRetrieve [relevant splits](https://www.pinecone.io/learn/what-is-similarity-search/) for any question using `similarity_search`.\n\n\n```typescript\nconst relevantDocs = await vectorStore.similaritySearch("What is task decomposition?");\n\nconsole.log(relevantDocs.length);\n\n// 4\n```\n\n\n### 2.2 Going Deeper\n\n#### 2.2.1 Retrieval\n\nVectorstores are commonly used for retrieval.\n\nBut, they are not the only option.\n\nFor example, SVMs (see thread [here](https://twitter.com/karpathy/status/1647025230546886658?s=20)) can also be used.\n\nLangChain [has many retrievers and retrieval methods](/docs/modules/data_connection/retrievers/) including, but not limited to, vectorstores.\n\nAll retrievers implement some common methods, such as `getRelevantDocuments()`.\n\n\n## 3. QA\n\n### 3.1 Getting started\n\nDistill the retrieved documents into an answer using an LLM (e.g., `gpt-3.5-turbo`) with `RetrievalQA` chain.\n\n\n```typescript\nimport { RetrievalQAChain } from "langchain/chains";\nimport { ChatOpenAI } from "langchain/chat_models/openai";\n\nconst model = new ChatOpenAI({ modelName: "gpt-3.5-turbo" });\nconst chain = RetrievalQAChain.fromLLM(model, vectorstore.asRetriever());\n\nconst response = await chain.call({\n  query: "What is task decomposition?"\n});\nconsole.log(response);\n\n/*\n  {\n    text: \'Task decomposition refers to the process of breaking down a larger task into smaller, more manageable subgoals. By decomposing a task, it becomes easier for an agent or system to handle complex tasks efficiently. Task decomposition can be done through various methods such as using prompting or task-specific instructions, or through human inputs. It helps in planning and organizing the steps required to complete a task effectively.\'\n  }\n*/\n```\n\n### 3.2 Going Deeper\n\n#### 3.2.1 Integrations\n\n`LLMs`\n\n* Browse LLM integrations and further documentation [here](/docs/modules/model_io/models/).\n\n#### 3.2.2 Customizing the prompt\n\nThe prompt in `RetrievalQA` chain can be customized as follows.\n\n\n```typescript\nimport { RetrievalQAChain } from "langchain/chains";\nimport { ChatOpenAI } from "langchain/chat_models/openai";\nimport { PromptTemplate } from "langchain/prompts";\n\nconst model = new ChatOpenAI({ modelName: "gpt-3.5-turbo" });\n\nconst template = `Use the following pieces of context to answer the question at the end.\nIf you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer.\nUse three sentences maximum and keep the answer as concise as possible.\nAlways say "thanks for asking!" at the end of the answer.\n{context}\nQuestion: {question}\nHelpful Answer:`;\n\nconst chain = RetrievalQAChain.fromLLM(model, vectorstore.asRetriever(), {\n  prompt: PromptTemplate.fromTemplate(template),\n});\n\nconst response = await chain.call({\n  query: "What is task decomposition?"\n});\n\nconsole.log(response);\n\n/*\n  {\n    text: \'Task decomposition is the process of breaking down a large task into smaller, more manageable subgoals. This allows for efficient handling of complex tasks and aids in planning and organizing the steps needed to achieve the overall goal. Thanks for asking!\'\n  }\n*/\n```\n\n\n#### 3.2.3 Returning source documents\n\nThe full set of retrieved documents used for answer distillation can be returned using `return_source_documents=True`.\n\n\n```typescript\nimport { RetrievalQAChain } from "langchain/chains";\nimport { ChatOpenAI } from "langchain/chat_models/openai";\n\nconst model = new ChatOpenAI({ modelName: "gpt-3.5-turbo" });\n\nconst chain = RetrievalQAChain.fromLLM(model, vectorstore.asRetriever(), {\n  returnSourceDocuments: true\n});\n\nconst response = await chain.call({\n  query: "What is task decomposition?"\n});\n\nconsole.log(response.sourceDocuments[0]);\n\n/*\nDocument {\n  pageContent: \'Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.\',\n  metadata: [Object]\n}\n*/\n```\n\n\n#### 3.2.4 Customizing retrieved docs in the LLM prompt\n\nRetrieved documents can be fed to an LLM for answer distillation in a few different ways.\n\n`stuff`, `refine`, and `map-reduce` chains for passing documents to an LLM prompt are well summarized [here](/docs/modules/chains/document/).\n\n`stuff` is commonly used because it simply "stuffs" all retrieved documents into the prompt.\n\nThe [loadQAChain](/docs/modules/chains/document/) methods are easy ways to pass documents to an LLM using these various approaches.\n\n\n```typescript\nimport { loadQAStuffChain } from "langchain/chains";\n\nconst stuffChain = loadQAStuffChain(model);\n\nconst stuffResult = await stuffChain.call({\n  input_documents: relevantDocs,\n  question: "What is task decomposition\n});\n\nconsole.log(stuffResult);\n/*\n{\n  text: \'Task decomposition is the process of breaking down a large task into smaller, more manageable subgoals or steps. This allows for efficient handling of complex tasks by focusing on one subgoal at a time. Task decomposition can be done through various methods such as using simple prompting, task-specific instructions, or human inputs.\'\n}\n*/\n```\n\n## 4. Chat\n\n### 4.1 Getting started\n\nTo keep chat history, we use a variant of the previous chain called a `ConversationalRetrievalQAChain`.\nFirst, specify a `Memory buffer` to track the conversation inputs / outputs.\n\n\n```typescript\nimport { ConversationalRetrievalQAChain } from "langchain/chains";\nimport { BufferMemory } from "langchain/memory";\nimport { ChatOpenAI } from "langchain/chat_models/openai";\n\nconst memory = new BufferMemory({\n  memoryKey: "chat_history",\n  returnMessages: true,\n});\n```\n\nNext, we initialize and call the chain:\n\n```typescript\nconst model = new ChatOpenAI({ modelName: "gpt-3.5-turbo" });\nconst chain = ConversationalRetrievalQAChain.fromLLM(model, vectorstore.asRetriever(), {\n  memory\n});\n\nconst result = await chain.call({\n  question: "What are some of the main ideas in self-reflection?"\n});\nconsole.log(result);\n\n/*\n{\n  text: \'Some main ideas in self-reflection include:\n\' +\n    \'\n\' +\n    \'1. Iterative Improvement: Self-reflection allows autonomous agents to improve by continuously refining past action decisions and correcting mistakes.\n\' +\n    \'\n\' +\n    \'2. Trial and Error: Self-reflection plays a crucial role in real-world tasks where trial and error are inevitable. It helps agents learn from failed trajectories and make adjustments for future actions.\n\' +\n    \'\n\' +\n    \'3. Constructive Criticism: Agents engage in constructive self-criticism of their big-picture behavior to identify areas for improvement.\n\' +\n    \'\n\' +\n    \'4. Decision and Strategy Refinement: Reflection on past decisions and strategies enables agents to refine their approach and make more informed choices.\n\' +\n    \'\n\' +\n    \'5. Efficiency and Optimization: Self-reflection encourages agents to be smart and efficient in their actions, aiming to complete tasks in the least number of steps.\n\' +\n    \'\n\' +\n    \'These ideas highlight the importance of self-reflection in enhancing performance and guiding future actions.\'\n}\n*/\n```\n\n\nThe `Memory buffer` has context to resolve `"it"` ("self-reflection") in the below question.\n\n\n```typescript\nconst followupResult = await chain.call({\n  question: "How does the Reflexion paper handle it?"\n});\nconsole.log(followupResult);\n\n/*\n{\n  text: "The Reflexion paper introduces a framework that equips agents with dynamic memory and self-reflection capabilities to improve their reasoning skills. The approach involves showing the agent two-shot examples, where each example consists of a failed trajectory and an ideal reflection on how to guide future changes in the agent\'s plan. These reflections are then added to the agent\'s working memory as context for querying a language model. The agent uses this self-reflection information to make decisions on whether to start a new trial or continue with the current plan."\n}\n*/\n```\n\n\n### 4.2 Going deeper\n\nThe [documentation](/docs/modules/chains/popular/chat_vector_db) on `ConversationalRetrievalQAChain` offers a few extensions, such as streaming and source documents.\n\n\n# Conversational Retrieval Agents\n\nThis is an agent specifically optimized for doing retrieval when necessary while holding a conversation and being able\nto answer questions based on previous dialogue in the conversation.\n\nTo start, we will set up the retriever we want to use, then turn it into a retriever tool. Next, we will use the high-level constructor for this type of agent.\nFinally, we will walk through how to construct a conversational retrieval agent from components.\n\n## The Retriever\n\nTo start, we need a retriever to use! The code here is mostly just example code. Feel free to use your own retriever and skip to the next section on creating a retriever tool.\n\n```typescript\nimport { FaissStore } from "langchain/vectorstores/faiss";\nimport { OpenAIEmbeddings } from "langchain/embeddings/openai";\nimport { TextLoader } from "langchain/document_loaders/fs/text";\nimport { RecursiveCharacterTextSplitter } from "langchain/text_splitter";\n\nconst loader = new TextLoader("state_of_the_union.txt");\nconst docs = await loader.load();\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 0\n});\n\nconst texts = await splitter.splitDocuments(docs);\n\nconst vectorStore = await FaissStore.fromDocuments(texts, new OpenAIEmbeddings());\n\nconst retriever = vectorStore.asRetriever();\n```\n\n## Retriever Tool\n\nNow we need to create a tool for our retriever. The main things we need to pass in are a `name` for the retriever as well as a `description`. These will both be used by the language model, so they should be informative.\n\n```typescript\nimport { createRetrieverTool } from "langchain/agents/toolkits";\n\nconst tool = createRetrieverTool(retriever, {\n  name: "search_state_of_union",\n  description: "Searches and returns documents regarding the state-of-the-union.",\n});\n```\n\n## Agent Constructor\n\nHere, we will use the high level `create_conversational_retrieval_agent` API to construct the agent.\nNotice that beside the list of tools, the only thing we need to pass in is a language model to use.\n\nUnder the hood, this agent is using the OpenAIFunctionsAgent, so we need to use an ChatOpenAI model.\n\n```typescript\nimport { createConversationalRetrievalAgent } from "langchain/agents/toolkits";\nimport { ChatOpenAI } from "langchain/chat_models/openai";\n\nconst model = new ChatOpenAI({\n  temperature: 0,\n});\n\nconst executor = await createConversationalRetrievalAgent(model, [tool], {\n  verbose: true,\n});\n```\n\nWe can now try it out!\n\n```typescript\nconst result = await executor.call({\n  input: "Hi, I\'m Bob!"\n});\n\nconsole.log(result);\n\n/*\n  {\n    output: \'Hello Bob! How can I assist you today?\',\n    intermediateSteps: []\n  }\n*/\n\nconst result2 = await executor.call({\n  input: "What\'s my name?"\n});\n\nconsole.log(result2);\n\n/*\n  { output: \'Your name is Bob.\', intermediateSteps: [] }\n*/\n\nconst result3 = await executor.call({\n  input: "What did the president say about Ketanji Brown Jackson in the most recent state of the union?"\n});\n\nconsole.log(result3);\n\n/*\n  {\n    output: "In the most recent state of the union, President Biden mentioned Ketanji Brown Jackson. He nominated her as a Circuit Court of Appeals judge and described her as one of the nation\'s top legal minds who will continue Justice Breyer\'s legacy of excellence. He mentioned that she has received a broad range of support, including from the Fraternal Order of Police and former judges appointed by Democrats and Republicans.",\n    intermediateSteps: [\n      {...}\n    ]\n  }\n*/\n\nconst result4 = await executor.call({\n  input: "How long ago did he nominate her?"\n});\n\nconsole.log(result4);\n\n/*\n  {\n    output: \'President Biden nominated Ketanji Brown Jackson four days before the most recent state of the union address.\',\n    intermediateSteps: []\n  }\n*/\n```\n\nNote that for the final call, the agent used previously retrieved information to answer the query and did not need to call the tool again!\n\nHere\'s a trace showing how the agent fetches documents to answer the question with the retrieval tool:\n\nhttps://smith.langchain.com/public/1e2b1887-ca44-4210-913b-a69c1b8a8e7e/r\n\n## Creating from components\n\nWhat actually is going on underneath the hood? Let\'s take a look so we can understand how to modify things going forward.\n\n### Memory\n\nIn this example, we want the agent to remember not only previous conversations, but also previous intermediate steps.\nFor that, we can use `OpenAIAgentTokenBufferMemory`. Note that if you want to change whether the agent remembers intermediate steps,\nhow the long the retained buffer is, or anything like that you should change this part.\n\n```typescript\nimport { OpenAIAgentTokenBufferMemory } from "langchain/agents/toolkits";\n\nconst memory = new OpenAIAgentTokenBufferMemory({\n  llm: model,\n  memoryKey: "chat_history",\n  outputKey: "output"\n});\n```\n\nYou should make sure `memoryKey` is set to `"chat_history"` and `outputKey` is set to `"output"` for the OpenAI functions agent.\nThis memory also has `returnMessages` set to `true` by default.\n\nYou can also load messages from prior conversations into this memory by initializing it with a pre-loaded chat history:\n\n```typescript\nimport { ChatOpenAI } from "langchain/chat_models/openai";\nimport { OpenAIAgentTokenBufferMemory } from "langchain/agents/toolkits";\nimport { HumanMessage, AIMessage } from "langchain/schema";\nimport { ChatMessageHistory } from "langchain/memory";\n\nconst previousMessages = [\n  new HumanMessage("My name is Bob"),\n  new AIMessage("Nice to meet you, Bob!"),\n];\n\nconst chatHistory = new ChatMessageHistory(previousMessages);\n\nconst memory = new OpenAIAgentTokenBufferMemory({\n  llm: new ChatOpenAI({}),\n  memoryKey: "chat_history",\n  outputKey: "output",\n  chatHistory,\n});\n```\n\n### Agent executor\n\nWe can recreate the agent executor directly with the `initializeAgentExecutorWithOptions` method.\nThis allows us to customize the agent\'s system message by passing in a `prefix` into `agentArgs`.\nImportantly, we must pass in `return_intermediate_steps: true` since we are recording that with our memory object.\n\n```typescript\nimport { initializeAgentExecutorWithOptions } from "langchain/agents";\n\nconst executor = await initializeAgentExecutorWithOptions(tools, llm, {\n  agentType: "openai-functions",\n  memory,\n  returnIntermediateSteps: true,\n  agentArgs: {\n    prefix:\n      prefix ??\n      `Do your best to answer the questions. Feel free to use any tools available to look up relevant information, only if necessary.`,\n  },\n});\n```\n'),ingest=async e=>{e.preventDefault(),n(!0);let o=await fetch("/api/retrieval/ingest",{method:"POST",body:JSON.stringify({text:t})});if(200===o.status)s("Uploaded!");else{let e=await o.json();e.error&&s(e.error)}n(!1)};return(0,o.jsxs)("form",{onSubmit:ingest,className:"flex w-full mb-4",children:[(0,o.jsx)("textarea",{className:"grow mr-8 p-4 rounded",value:t,onChange:e=>s(e.target.value)}),(0,o.jsxs)("button",{type:"submit",className:"shrink-0 px-8 py-4 bg-sky-600 rounded w-28",children:[(0,o.jsxs)("div",{role:"status",className:"".concat(e?"":"hidden"," flex justify-center"),children:[(0,o.jsxs)("svg",{"aria-hidden":"true",className:"w-6 h-6 text-white animate-spin dark:text-white fill-sky-800",viewBox:"0 0 100 101",fill:"none",xmlns:"http://www.w3.org/2000/svg",children:[(0,o.jsx)("path",{d:"M100 50.5908C100 78.2051 77.6142 100.591 50 100.591C22.3858 100.591 0 78.2051 0 50.5908C0 22.9766 22.3858 0.59082 50 0.59082C77.6142 0.59082 100 22.9766 100 50.5908ZM9.08144 50.5908C9.08144 73.1895 27.4013 91.5094 50 91.5094C72.5987 91.5094 90.9186 73.1895 90.9186 50.5908C90.9186 27.9921 72.5987 9.67226 50 9.67226C27.4013 9.67226 9.08144 27.9921 9.08144 50.5908Z",fill:"currentColor"}),(0,o.jsx)("path",{d:"M93.9676 39.0409C96.393 38.4038 97.8624 35.9116 97.0079 33.5539C95.2932 28.8227 92.871 24.3692 89.8167 20.348C85.8452 15.1192 80.8826 10.7238 75.2124 7.41289C69.5422 4.10194 63.2754 1.94025 56.7698 1.05124C51.7666 0.367541 46.6976 0.446843 41.7345 1.27873C39.2613 1.69328 37.813 4.19778 38.4501 6.62326C39.0873 9.04874 41.5694 10.4717 44.0505 10.1071C47.8511 9.54855 51.7191 9.52689 55.5402 10.0491C60.8642 10.7766 65.9928 12.5457 70.6331 15.2552C75.2735 17.9648 79.3347 21.5619 82.5849 25.841C84.9175 28.9121 86.7997 32.2913 88.1811 35.8758C89.083 38.2158 91.5421 39.6781 93.9676 39.0409Z",fill:"currentFill"})]}),(0,o.jsx)("span",{className:"sr-only",children:"Loading..."})]}),(0,o.jsx)("span",{className:e?"hidden":"",children:"Upload"})]})]})}}}]);